=======================
 Tutorial Introduction
=======================

This introduction will go through how to retrieve device information, construct
physical addresses, issue vectorized IO, retrieve and mange media state, and
introduce the virtual block abstraction.

It will be using the command-line interface (CLI), and provide notes for the
corresponding parts of the C API. It is assumed throughout that an Open-Channel
SSD available at the path ``/dev/nvme0n1`` and that liblightnvm is installed on
the system.

To begin with, knowing the physical geometry of a device is essential for
working with physical addressing.

Obtaining device information
============================

Device information is obtained by invoking::

  nvm_dev info /dev/nvme0n1

Which will yield device information as shown below::

  dev {
   verid(0x02), beid(0x01),
   path(/dev/nvme0n1), name(nvme0n1), fd(3),
   ssw(12), pmode(1),
   erase_naddrs_max(64), read_naddrs_max(64), write_naddrs_max(64),
   meta_mode(0),
   bbts_cached(0)
  },
  dev-geo {
   nchannels(16), nluns(8), nplanes(2),
   nblocks(1020), npages(512), nsectors(4),
   page_nbytes(16384), sector_nbytes(4096), meta_nbytes(16),
   tbytes(2190433320960b:2088960Mb),
  }
  dev-ppaf {
    ch_off(25),  ch_len(04),
   lun_off(22), lun_len(03),
    pl_off(02),  pl_len(01),
   blk_off(12), blk_len(10),
    pg_off(03),  pg_len(09),
   sec_off(00), sec_len(02),
  }
  dev-ppaf_mask {
    ch(0000000000000000000000000000000000011110000000000000000000000000),
   lun(0000000000000000000000000000000000000001110000000000000000000000),
    pl(0000000000000000000000000000000000000000000000000000000000000100),
   blk(0000000000000000000000000000000000000000001111111111000000000000),
    pg(0000000000000000000000000000000000000000000000000000111111111000),
   sec(0000000000000000000000000000000000000000000000000000000000000011)
  }

The parts involved from the C API are: ``nvm_dev_open`` to obtain a device
handle, ``nvm_dev_pr`` to produce the output above, and lastly
``nvm_dev_close`` to terminate the handle properly.

When using the C API, values and structures are retrieved using the attribute
getters ``nvm_dev_get_*`` e.g. use ``nvm_dev_get_geo`` to obtain the geometry
of a given device.

Physical Addressing
===================

Most of the library takes one or more physical addresses as parameter.

Generic format
--------------

The physical addresses are represented in generic format by the data-structure
``struct nvm_addr``. One can construct an address by specifying the relative
location within the device geometry down to the granularity of a sector.

Example: Construct an address for sector 3 within page 11 in block 200 on
plane 0 of LUN 1 in channel 4::

  nvm_addr from_geo /dev/nvme0n1 4 1 0 200 10 3

Yielding::

  (0x04010003000a00c8){ ch(04), lun(01), pl(0), blk(0200), pg(010), sec(3) }

The above hexidecimal-value can be given to any CLI command taking an ``addr``
as parameter.

.. NOTE:: Addresses are zero-indexed, so channel 4 is the fifth channel

.. NOTE:: C API address construction is done by assigning the members of ``struct nvm_addr``

Device format
-------------

As the output from the device information shows, then there is a notion of a
device format. The library user need not be concerned with the device format as
the translation to device format is handled by the library for every part of
the interface with the exception of the low-level command-interface
``nvm_cmd``.

However, if one needs an address on device format for ``nvm_cmd`` or another
tool such as ``nvme-cli``, then the generic-format can be converted to
device format using::

  nvm_addr gen2dev /dev/nvme0n1 0x04010003000a00c8

Yielding::

  gen-addr(0x04010003000a00c8){ ch(04), lun(01), pl(0), blk(0200), pg(010), sec(3) }
  dev-addr(0x00000000084c8053)

.. NOTE:: C API address format conversion is done using ``nvm_addr_gen2dev``

Address scope
-------------

An address specifies the relative location of all parts of the geometry,
channel, lun, plane, block, page and sector. However, not all parts of the
library uses all location information. Most common uses are:

LUN address
  Specify channel and LUN within the channel

Block address
  Specify channel, LUN within the channel, plane within the LUN, and block
  within the plane

Sector address
  Specify all relative locations of the geometry

Vectorized IO to NAND media
===========================

With the basics of obtaining device information and constructing addresses in
place one can dive into the task of constructing commands for doing vectorized
IO.

As the section on background information describes, then there are handful of
constraints to handle for IOs to NAND media to succeed.

Erase before write
------------------

The first constraint to handle is that a block must be **erased** before it can be
**written**. We do so by constructing block-addresses for all blocks within a plane::

    nvm_addr from_geo 0 0 0 10 0 0
    nvm_addr from_geo 0 0 1 10 0 0

Yielding::

    (0x000000000000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(0) }
    (0x000001000000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(0) }

With the addresses we can now construct a single command with two addresses and
send of the erase::

    nvm_addr erase /dev/nvme0n1 0x000000000000000a 0x000001000000000a

On success, yielding::

    ** nvm_addr_erase(...) : pmode(0x1)
    (0x000000000000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(0) }
    (0x000001000000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(0) }

On error, yielding::

    ** nvm_addr_erase(...) : pmode(0x1)
    (0x000000000000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(0) }
    (0x000001000000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(0) }
    nvm_addr_erase: Input/output error
    nvm_ret { result(0xff), status(3) }

If an erase fails as above then it is because a block is bad. The
bad-block-table interface (nvm_bbt) is provided to communicate and update media
state. Introduction to the bad-block-table is given in a later section.

.. NOTE:: C API for performing erases using block-adressing is done with ``nvm_addr_erase``

Write
-----

The two primary constraints for issuing writes are that they must be at the
granularity of a full flash page and contiguous within a block.

.. NOTE :: C API for performing write using vectorized IO with addressing at
    sector-level is done using ``nvm_addr_write``, note that the payload must
    be aligned to sector size, the helper function ``nvm_buf_alloc`` is
    provided for convenience

Minimum write
~~~~~~~~~~~~~

For the geometry in figure X, a full flash page is four sectors of each 4096
bytes, a command satisfying the minimum-write constraint thus contains four
addresses with a payload of 16384 bytes of data. The command can constructed
via the CLI as::

    nvm_addr write /dev/nvme0n1 \
    0x000000000000000a 0x000000010000000a 0x000000020000000a 0x000000030000000a

The CLI creates an arbitrary payload, so we do not concern us with the
payload at this point.

The result of the command is::

    nvm_addr_write(...) : pmode(0x1)
    (0x000000000000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(0) }
    (0x000000010000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(1) }
    (0x000000020000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(2) }
    (0x000000030000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(3) }
    nvm_addr_write: Input/output error
    nvm_ret { result(0x2), status(0) }

An unexpected write error occured, the constraints are satisfied, so what goes
wrong? The issue in this case is that the command is constructed using
plane-mode (specifically ``pmode(0x1)``).

This introduces an additional constraint that writes must be performed to
the block accross all planes. One can choose to disable the plane-mode, which is
done by setting environment var ``NVM_CLI_PMODE="0"`` or by constructing a
command satisfying the plane-mode constraint::

    nvm_addr write /dev/nvme0n1 \
    0x000000000000000a 0x000000010000000a 0x000000020000000a 0x000000030000000a
    0x000001000000000a 0x000001010000000a 0x000001020000000a 0x000001030000000a 

Yielding without error::

    ** nvm_addr_write(...) : pmode(0x1)
    (0x000000000000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(0) }
    (0x000000010000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(1) }
    (0x000000020000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(2) }
    (0x000000030000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(3) }
    (0x000001000000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(0) }
    (0x000001010000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(1) }
    (0x000001020000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(2) }
    (0x000001030000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(3) }

The plane-mode allows for multiple writes to be done in parallel across the
planes.

Maximum write
~~~~~~~~~~~~~

An improvement of round-trip-time can be obtained by increasing the amount of
work done by a single command, that is, increase the number of addresses.

There is an upper limit, write-naddrs-max,  as can be seen in figure X, and
retrieved from the device. In our case we can construct a command with 64
addresses.

Abiding to all of the above mentioned constraints a write command can be
constructed as::

    nvm_addr write /dev/nvme0n1 \
    0x000000000001000a 0x000000010001000a 0x000000020001000a 0x000000030001000a \
    0x000001000001000a 0x000001010001000a 0x000001020001000a 0x000001030001000a \
    0x000000000002000a 0x000000010002000a 0x000000020002000a 0x000000030002000a \
    0x000001000002000a 0x000001010002000a 0x000001020002000a 0x000001030002000a \
    0x000000000003000a 0x000000010003000a 0x000000020003000a 0x000000030003000a \
    0x000001000003000a 0x000001010003000a 0x000001020003000a 0x000001030003000a \
    0x000000000004000a 0x000000010004000a 0x000000020004000a 0x000000030004000a \
    0x000001000004000a 0x000001010004000a 0x000001020004000a 0x000001030004000a \
    0x000000000005000a 0x000000010005000a 0x000000020005000a 0x000000030005000a \
    0x000001000005000a 0x000001010005000a 0x000001020005000a 0x000001030005000a \
    0x000000000006000a 0x000000010006000a 0x000000020006000a 0x000000030006000a \
    0x000001000006000a 0x000001010006000a 0x000001020006000a 0x000001030006000a \
    0x000000000007000a 0x000000010007000a 0x000000020007000a 0x000000030007000a \
    0x000001000007000a 0x000001010007000a 0x000001020007000a 0x000001030007000a \
    0x000000000008000a 0x000000010008000a 0x000000020008000a 0x000000030008000a \
    0x000001000008000a 0x000001010008000a 0x000001020008000a 0x000001030008000a

Successfully yielding::

    ** nvm_addr_write(...) : pmode(0x1)
    (0x000000000001000a){ ch(00), lun(00), pl(0), blk(0010), pg(001), sec(0) }
    (0x000000010001000a){ ch(00), lun(00), pl(0), blk(0010), pg(001), sec(1) }
    (0x000000020001000a){ ch(00), lun(00), pl(0), blk(0010), pg(001), sec(2) }
    (0x000000030001000a){ ch(00), lun(00), pl(0), blk(0010), pg(001), sec(3) }
    (0x000001000001000a){ ch(00), lun(00), pl(1), blk(0010), pg(001), sec(0) }
    (0x000001010001000a){ ch(00), lun(00), pl(1), blk(0010), pg(001), sec(1) }
    (0x000001020001000a){ ch(00), lun(00), pl(1), blk(0010), pg(001), sec(2) }
    (0x000001030001000a){ ch(00), lun(00), pl(1), blk(0010), pg(001), sec(3) }
    (0x000000000002000a){ ch(00), lun(00), pl(0), blk(0010), pg(002), sec(0) }
    (0x000000010002000a){ ch(00), lun(00), pl(0), blk(0010), pg(002), sec(1) }
    (0x000000020002000a){ ch(00), lun(00), pl(0), blk(0010), pg(002), sec(2) }
    (0x000000030002000a){ ch(00), lun(00), pl(0), blk(0010), pg(002), sec(3) }
    (0x000001000002000a){ ch(00), lun(00), pl(1), blk(0010), pg(002), sec(0) }
    (0x000001010002000a){ ch(00), lun(00), pl(1), blk(0010), pg(002), sec(1) }
    (0x000001020002000a){ ch(00), lun(00), pl(1), blk(0010), pg(002), sec(2) }
    (0x000001030002000a){ ch(00), lun(00), pl(1), blk(0010), pg(002), sec(3) }
               ... output for pages 3-7 omitted for brevity ...
    (0x000000000008000a){ ch(00), lun(00), pl(0), blk(0010), pg(008), sec(0) }
    (0x000000010008000a){ ch(00), lun(00), pl(0), blk(0010), pg(008), sec(1) }
    (0x000000020008000a){ ch(00), lun(00), pl(0), blk(0010), pg(008), sec(2) }
    (0x000000030008000a){ ch(00), lun(00), pl(0), blk(0010), pg(008), sec(3) }
    (0x000001000008000a){ ch(00), lun(00), pl(1), blk(0010), pg(008), sec(0) }
    (0x000001010008000a){ ch(00), lun(00), pl(1), blk(0010), pg(008), sec(1) }
    (0x000001020008000a){ ch(00), lun(00), pl(1), blk(0010), pg(008), sec(2) }
    (0x000001030008000a){ ch(00), lun(00), pl(1), blk(0010), pg(008), sec(3) }

Using vectorized IO we have with a single command successfully written a
payload of 64 x 4096 bytes = 256 KB.

Read
----

Reads have fewer constraints than writes. The granularity of a read is a single
sector (the smallest addressable unit) and can be performed non-contiguously.

The primary constraint for a read to adhere to is that the block which is read
from must be closed. That is, all pages within the block must have been
written. It might be that the constraint can be relaxed where only N pages
ahead of the read must have been written instead of all pages in the block. The
challenge with relaxing the constraint is that N is often an unknown size.

We have so far written a total of nine pages (across two planes), the first
page in one command, the remaining eight pages in a second command. Thus we
have 503 pages that need to be written before we can start reading.

Specifying the 503 x nplanes x nsectors = 4024 addresses via the CLI is
tedious, we will therefore take a sneak peak at virtual blocks and execute::

  nvm_vblk line_erase /dev/nvme0n1 0 0 0 0 10
  nvm_vblk line_write /dev/nvme0n1 0 0 0 0 10

What these two commands actually do will be described in the following section
on virtual blocks. For now all we need to know is that the block is now fully
written / closed and we can start reading from it.

.. NOTE :: C API for performing write using vectorized IO with addressing at
  sector-level is done using ``nvm_addr_read``, the received payload must be
  stored in a sector-aligned buffer, the helper function ``nvm_buf_alloc`` is
  provided for convenience

Minimal Read
~~~~~~~~~~~~

We can read a single sector::

  nvm_addr read /dev/nvme0n1 0x000000000000000a

Yielding::

  ** nvm_addr_read(...) : pmode(0x1)
  (0x000000000000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(0) }

Setting the environment var ``NVM_CLI_BUF_PR``, will dump the read payload to
stdout, yielding::

  ** nvm_addr_read(...) : pmode(0x1)
  (0x000000000000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(0) }
  ** Read buffer:
  ** NVM_BUF_PR - BEGIN **
  i[0,31]:  A B C D E F G H I J K L M N O P Q R S T U V W X Y Z A B C D E F
  i[32,63]:  G H I J K L M N O P Q R S T U V W X Y Z A B C D E F G H I J K L
  i[64,95]:  M N O P Q R S T U V W X Y Z A B C D E F G H I J K L M N O P Q R
  i[96,127]:  S T U V W X Y Z A B C D E F G H I J K L M N O P Q R S T U V W X
               ... output for bytes 128 - 4063 omitted for brevity ...
  i[4064,4095]:  I J K L M N O P Q R S T U V W X Y Z A B C D E F G H I J K L M N
  ** NVM_BUF_PR - END **

Maximum Read
~~~~~~~~~~~~

Same as a write, a read has an upper bound on the number of addresses in a
single command::

  nvm_addr read /dev/nvme0n1 \
  0x000000000000000a 0x000000010000000a 0x000000020000000a 0x000000030000000a \
  0x000001000000000a 0x000001010000000a 0x000001020000000a 0x000001030000000a \
  0x000000000001000a 0x000000010001000a 0x000000020001000a 0x000000030001000a \
  0x000001000001000a 0x000001010001000a 0x000001020001000a 0x000001030001000a \
  0x000000000002000a 0x000000010002000a 0x000000020002000a 0x000000030002000a \
  0x000001000002000a 0x000001010002000a 0x000001020002000a 0x000001030002000a \
  0x000000000003000a 0x000000010003000a 0x000000020003000a 0x000000030003000a \
  0x000001000003000a 0x000001010003000a 0x000001020003000a 0x000001030003000a \
  0x000000000004000a 0x000000010004000a 0x000000020004000a 0x000000030004000a \
  0x000001000004000a 0x000001010004000a 0x000001020004000a 0x000001030004000a \
  0x000000000005000a 0x000000010005000a 0x000000020005000a 0x000000030005000a \
  0x000001000005000a 0x000001010005000a 0x000001020005000a 0x000001030005000a \
  0x000000000006000a 0x000000010006000a 0x000000020006000a 0x000000030006000a \
  0x000001000006000a 0x000001010006000a 0x000001020006000a 0x000001030006000a \
  0x000000000007000a 0x000000010007000a 0x000000020007000a 0x000000030007000a \
  0x000001000007000a 0x000001010007000a 0x000001020007000a 0x000001030007000a

Yielding::

  ** nvm_addr_read(...) : pmode(0x1)
  (0x000000000000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(0) }
  (0x000000010000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(1) }
  (0x000000020000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(2) }
  (0x000000030000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(3) }
  (0x000001000000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(0) }
  (0x000001010000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(1) }
  (0x000001020000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(2) }
  (0x000001030000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(3) }
  (0x000000000001000a){ ch(00), lun(00), pl(0), blk(0010), pg(001), sec(0) }
  (0x000000010001000a){ ch(00), lun(00), pl(0), blk(0010), pg(001), sec(1) }
  (0x000000020001000a){ ch(00), lun(00), pl(0), blk(0010), pg(001), sec(2) }
  (0x000000030001000a){ ch(00), lun(00), pl(0), blk(0010), pg(001), sec(3) }
  (0x000001000001000a){ ch(00), lun(00), pl(1), blk(0010), pg(001), sec(0) }
  (0x000001010001000a){ ch(00), lun(00), pl(1), blk(0010), pg(001), sec(1) }
  (0x000001020001000a){ ch(00), lun(00), pl(1), blk(0010), pg(001), sec(2) }
  (0x000001030001000a){ ch(00), lun(00), pl(1), blk(0010), pg(001), sec(3) }
               ... output for pages 3-7 omitted for brevity ...
  (0x000000000007000a){ ch(00), lun(00), pl(0), blk(0010), pg(007), sec(0) }
  (0x000000010007000a){ ch(00), lun(00), pl(0), blk(0010), pg(007), sec(1) }
  (0x000000020007000a){ ch(00), lun(00), pl(0), blk(0010), pg(007), sec(2) }
  (0x000000030007000a){ ch(00), lun(00), pl(0), blk(0010), pg(007), sec(3) }
  (0x000001000007000a){ ch(00), lun(00), pl(1), blk(0010), pg(007), sec(0) }
  (0x000001010007000a){ ch(00), lun(00), pl(1), blk(0010), pg(007), sec(1) }
  (0x000001020007000a){ ch(00), lun(00), pl(1), blk(0010), pg(007), sec(2) }
  (0x000001030007000a){ ch(00), lun(00), pl(1), blk(0010), pg(007), sec(3) }

Non-Contiguous Read
~~~~~~~~~~~~~~~~~~~

Reading pages 500, 200, 0, and 6 across planes::

  nvm_addr read /dev/nvme0n1 \
  0x0000000001f4000a 0x0000000101f4000a 0x0000000201f4000a 0x0000000301f4000a \
  0x0000010001f4000a 0x0000010101f4000a 0x0000010201f4000a 0x0000010301f4000a \
  0x0000000000c8000a 0x0000000100c8000a 0x0000000200c8000a 0x0000000300c8000a \
  0x0000010000c8000a 0x0000010100c8000a 0x0000010200c8000a 0x0000010300c8000a \
  0x000000000000000a 0x000000010000000a 0x000000020000000a 0x000000030000000a \
  0x000001000000000a 0x000001010000000a 0x000001020000000a 0x000001030000000a \
  0x000000000006000a 0x000000010006000a 0x000000020006000a 0x000000030006000a \
  0x000001000006000a 0x000001010006000a 0x000001020006000a 0x000001030006000a

Successfully yielding::

  ** nvm_addr_read(...) : pmode(0x1)
  (0x0000000001f4000a){ ch(00), lun(00), pl(0), blk(0010), pg(500), sec(0) }
  (0x0000000101f4000a){ ch(00), lun(00), pl(0), blk(0010), pg(500), sec(1) }
  (0x0000000201f4000a){ ch(00), lun(00), pl(0), blk(0010), pg(500), sec(2) }
  (0x0000000301f4000a){ ch(00), lun(00), pl(0), blk(0010), pg(500), sec(3) }
  (0x0000010001f4000a){ ch(00), lun(00), pl(1), blk(0010), pg(500), sec(0) }
  (0x0000010101f4000a){ ch(00), lun(00), pl(1), blk(0010), pg(500), sec(1) }
  (0x0000010201f4000a){ ch(00), lun(00), pl(1), blk(0010), pg(500), sec(2) }
  (0x0000010301f4000a){ ch(00), lun(00), pl(1), blk(0010), pg(500), sec(3) }
  (0x0000000000c8000a){ ch(00), lun(00), pl(0), blk(0010), pg(200), sec(0) }
  (0x0000000100c8000a){ ch(00), lun(00), pl(0), blk(0010), pg(200), sec(1) }
  (0x0000000200c8000a){ ch(00), lun(00), pl(0), blk(0010), pg(200), sec(2) }
  (0x0000000300c8000a){ ch(00), lun(00), pl(0), blk(0010), pg(200), sec(3) }
  (0x0000010000c8000a){ ch(00), lun(00), pl(1), blk(0010), pg(200), sec(0) }
  (0x0000010100c8000a){ ch(00), lun(00), pl(1), blk(0010), pg(200), sec(1) }
  (0x0000010200c8000a){ ch(00), lun(00), pl(1), blk(0010), pg(200), sec(2) }
  (0x0000010300c8000a){ ch(00), lun(00), pl(1), blk(0010), pg(200), sec(3) }
  (0x000000000000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(0) }
  (0x000000010000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(1) }
  (0x000000020000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(2) }
  (0x000000030000000a){ ch(00), lun(00), pl(0), blk(0010), pg(000), sec(3) }
  (0x000001000000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(0) }
  (0x000001010000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(1) }
  (0x000001020000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(2) }
  (0x000001030000000a){ ch(00), lun(00), pl(1), blk(0010), pg(000), sec(3) }
  (0x000000000006000a){ ch(00), lun(00), pl(0), blk(0010), pg(006), sec(0) }
  (0x000000010006000a){ ch(00), lun(00), pl(0), blk(0010), pg(006), sec(1) }
  (0x000000020006000a){ ch(00), lun(00), pl(0), blk(0010), pg(006), sec(2) }
  (0x000000030006000a){ ch(00), lun(00), pl(0), blk(0010), pg(006), sec(3) }
  (0x000001000006000a){ ch(00), lun(00), pl(1), blk(0010), pg(006), sec(0) }
  (0x000001010006000a){ ch(00), lun(00), pl(1), blk(0010), pg(006), sec(1) }
  (0x000001020006000a){ ch(00), lun(00), pl(1), blk(0010), pg(006), sec(2) }
  (0x000001030006000a){ ch(00), lun(00), pl(1), blk(0010), pg(006), sec(3) }

It is worth mentioning that the vectorized reads can be non-contiguous not only
within a block but also scattered across different different blocks, in
different LUNs, and channels.

However, when using plane-mode ensure that addresses are constructed across
planes and all sectors are read as in the example above.

Virtual Block
=============

The physical addressing interface ``nvm_addr`` provides full control over the
construction of vectorized IO commands. It is known that with great power comes
great responsibility, responsibility which increase the cognitive load on the
developer when integrating vectorized IO directly at the application level.

liblightnvm introduces a pure software abstraction, a virtual block, to reduce
the cognitive load for application developers.

A virtual block behaves as a physical, that is, the constraints of working with
NAND media also apply to a virtual block. However, the abstraction encapsulates
the command and address construction of parallel vectorized IO and exposes a
flat address space which is read/written in a manner equivalent to the
read/write primitives offered by libc.

A virtual block will at a minimum consist of the given block address across all
planes. So it by default encapsulates concerns regarding plane-mode
constraints.

We erase, write, and read the virtual block::

  nvm_vblk set_erase /dev/nvme1n1 0x0000000000000002
  nvm_vblk set_write /dev/nvme1n1 0x0000000000000002
  nvm_vblk set_read /dev/nvme1n1 0x0000000000000002

Yielding::

  ** nvm_vblk_erase(...):
  vblk {
   nbytes(16777216b:16Mb),
  }
  vblk-naddrs(1) {
   00: (0x0000000000000002){ ch(00), lun(00), pl(0), blk(0002), pg(000), sec(0) }
  }
  Ran nvm_vblk_erase, elapsed wall-clock: 0.025978
  ** nvm_vblk_write(...):
  vblk {
   nbytes(16777216b:16Mb),
  }
  vblk-naddrs(1) {
   00: (0x0000000000000002){ ch(00), lun(00), pl(0), blk(0002), pg(000), sec(0) }
  }
  Ran nvm_buf_alloc, elapsed wall-clock: 0.023954
  Ran nvm_vblk_write, elapsed wall-clock: 0.934091
  ** nvm_vblk_read(...):
  vblk {
   nbytes(16777216b:16Mb),
  }
  vblk-naddrs(1) {
   00: (0x0000000000000002){ ch(00), lun(00), pl(0), blk(0002), pg(000), sec(0) }
  }
  Ran nvm_buf_alloc, elapsed wall-clock: 0.000014
  Ran nvm_vblk_read, elapsed wall-clock: 0.153033

Using physical block sets
-------------------------

To obtain parallel IO, in addition to plane-mode, we construct a virtual block
consisting of multiple physical blocks on distinct parallel units (LUNs). As an
example, we create a virtual block, consisting of a block from the first LUN of
each channel on the device::

	(0x0000000000000002){ ch(00), lun(00), pl(0), blk(0002), pg(000), sec(0) }
	(0x0a0700000000014d){ ch(10), lun(07), pl(0), blk(0333), pg(000), sec(0) }
	(0x0301000000000014){ ch(03), lun(01), pl(0), blk(0020), pg(000), sec(0) }
	(0x0500000000000190){ ch(05), lun(00), pl(0), blk(0400), pg(000), sec(0) }

We erase, write and read the virtual block::

  nvm_vblk set_erase /dev/nvme1n1 0x0000000000000002 0x0a0700000000014d 0x0301000000000014 0x0500000000000190
  nvm_vblk set_write /dev/nvme1n1 0x0000000000000002 0x0a0700000000014d 0x0301000000000014 0x0500000000000190
  nvm_vblk set_read /dev/nvme1n1 0x0000000000000002 0x0a0700000000014d 0x0301000000000014 0x0500000000000190

Yielding::

  ** nvm_vblk_erase(...):
  vblk {
   nbytes(67108864b:64Mb),
  }
  vblk-naddrs(4) {
   00: (0x0000000000000002){ ch(00), lun(00), pl(0), blk(0002), pg(000), sec(0) }
   01: (0x0a0700000000014d){ ch(10), lun(07), pl(0), blk(0333), pg(000), sec(0) }
   02: (0x0301000000000014){ ch(03), lun(01), pl(0), blk(0020), pg(000), sec(0) }
   03: (0x0500000000000190){ ch(05), lun(00), pl(0), blk(0400), pg(000), sec(0) }
  }
  Ran nvm_vblk_erase, elapsed wall-clock: 0.027606
  ** nvm_vblk_write(...):
  vblk {
   nbytes(67108864b:64Mb),
  }
  vblk-naddrs(4) {
   00: (0x0000000000000002){ ch(00), lun(00), pl(0), blk(0002), pg(000), sec(0) }
   01: (0x0a0700000000014d){ ch(10), lun(07), pl(0), blk(0333), pg(000), sec(0) }
   02: (0x0301000000000014){ ch(03), lun(01), pl(0), blk(0020), pg(000), sec(0) }
   03: (0x0500000000000190){ ch(05), lun(00), pl(0), blk(0400), pg(000), sec(0) }
  }
  Ran nvm_buf_alloc, elapsed wall-clock: 0.071390
  Ran nvm_vblk_write, elapsed wall-clock: 0.980600
  ** nvm_vblk_read(...):
  vblk {
   nbytes(67108864b:64Mb),
  }
  vblk-naddrs(4) {
   00: (0x0000000000000002){ ch(00), lun(00), pl(0), blk(0002), pg(000), sec(0) }
   01: (0x0a0700000000014d){ ch(10), lun(07), pl(0), blk(0333), pg(000), sec(0) }
   02: (0x0301000000000014){ ch(03), lun(01), pl(0), blk(0020), pg(000), sec(0) }
   03: (0x0500000000000190){ ch(05), lun(00), pl(0), blk(0400), pg(000), sec(0) }
  }
  Ran nvm_buf_alloc, elapsed wall-clock: 0.000012
  Ran nvm_vblk_read, elapsed wall-clock: 0.167836

This demonstrates a weak-scaling experiment, increasing the workload
proportionally with the parallel units consumes approxiately the same amount of
wall-clock time, thus achieving near linear speedup by utilizing parallel units
on the device.

Using physical block line
-------------------------

Bad-Block-Table
===============

The state of the underlying media is exposed to the user through the
bad-block-interface (BBT). A bad-block table exists for each LUN, retrieving
the BBT for LUN 3 on channel 2 is done by executing::

  nvm_bbt get /dev/nvme0n1 2 3 | grep -vi free

The command outputs the entire table so we filter out the less interesting free
entries, thus yielding::

  ** nvm_bbt_get(...):
  bbt {
   addr(0x0203000000000000){ ch(02), lun(03), pl(0), blk(0000), pg(000), sec(0) }
   nblks(2040),
   npl_blks(1020) {
    blk(0041): [  BAD(1)  BAD(1) ]
   },
    nbad(1),
    ngbad(0),
    ndmrk(0),
    nhmrk(0)
   }
  }

A given block can be in one of the following states: FREE, BAD, GROWN BAD,
DEVICE MARKED/RESERVED, and HOST MARKED/RESERVED. State can be changed by e.g.
marking block 42 as HOST RESERVED::

  sudo nvm_bbt mark_f /dev/nvme0n1 0x0203000000000042

Yielding::

  ** nvm_bbt_mark(...):
  (0x020300000000002a){ ch(02), lun(03), pl(0), blk(0042), pg(000), sec(0) }

.. NOTE :: Only the system administrator is able to make state changes to the
  BBT

Retrieving the BBT after the state change yields::

  ** nvm_bbt_get(...):
  bbt {
   addr(0x0203000000000000){ ch(02), lun(03), pl(0), blk(0000), pg(000), sec(0) }
   nblks(2040),
   npl_blks(1020) {
    blk(0041): [  BAD(1)  BAD(1) ]
    blk(0042): [ HMRK(8) HMRK(8) ]
   },
    nbad(1),
    ngbad(0),
    ndmrk(0),
    nhmrk(1)
   }
  }

.. NOTE :: C API for retrieving and modifying the BBT is done using
  ``nvm_bbt_get`` which return a ``const struct nvm_bbt*``. To modify it, make
  a copy with ``nvm_bbt_alloc_cp``, and persist it with ``nvm_bbt_set``.
